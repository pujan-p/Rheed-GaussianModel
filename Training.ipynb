{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables: \n",
    "\n",
    "YES = 1\n",
    "NO = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive:\n",
    "using_google_drive = NO\n",
    "\n",
    "if using_google_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import least_squares\n",
    "from dask import delayed, compute\n",
    "from dask.distributed import Client\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "output_scaler = StandardScaler()\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read H5 Data File:\n",
    "DATA_DIR = '/mnt/Research/Data/' # Change to your DATA PATH\n",
    "\n",
    "RHEED_DATA_FILE = DATA_DIR + 'RHEED_4848_test6.h5'\n",
    "spot = 'spot_2'\n",
    "h5 = h5py.File(RHEED_DATA_FILE, 'r')\n",
    "\n",
    "raw_data = []\n",
    "for growth in h5.keys():\n",
    "    raw_data.extend(h5[growth][spot])\n",
    "raw_data = np.array(raw_data).astype(np.float32)\n",
    "\n",
    "normalized_images = []\n",
    "for image in raw_data:\n",
    "    normalized_images.append(image / np.max(image))\n",
    "normalized_images = np.array(normalized_images).astype(np.float32)\n",
    "\n",
    "print(f'[Normalized Images Shape]: {normalized_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Data Array:\n",
    "validate_data_array = YES\n",
    "\n",
    "if validate_data_array:\n",
    "    rand_int = np.random.randint(low=0, high=normalized_images.shape[0])\n",
    "    print(f'[Normalized Image #{rand_int}]:')\n",
    "    plt.imshow(normalized_images[rand_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for estimating labels\n",
    "\n",
    "# generate 2d Gaussian from its parameters\n",
    "# x, y = x-coord, y-coord\n",
    "# A = amplitude\n",
    "# x0, y0 = mean-x, mean-y\n",
    "# sigma_x, sigma_y = std.-dev.-x, std.-dev.-y\n",
    "def gaussian_2D(x, y, A, x0, y0, sigma_x, sigma_y):\n",
    "    return A * np.exp(-((x - x0)**2 / (2 * sigma_x**2) + (y - y0)**2 / (2 * sigma_y**2)))\n",
    "\n",
    "# Initial guess for each parameter\n",
    "# data = normalized image\n",
    "def add_guess(data):\n",
    "    A_guess = np.max(data)\n",
    "    x0_guess, y0_guess = np.unravel_index(np.argmax(data), data.shape)\n",
    "    sigma_x_guess = sigma_y_guess = np.std(data)\n",
    "    return [A_guess, x0_guess, y0_guess, sigma_x_guess, sigma_y_guess]\n",
    "\n",
    "# Compute residuals\n",
    "# params = A, x0, y0, sigma_x, sigma_y\n",
    "# x, y  = x-coord, y-coord\n",
    "# data = normalized image\n",
    "def residuals(params, x, y, data):\n",
    "    A, x0, y0, sigma_x, sigma_y = params\n",
    "    model = gaussian_2D(x, y, A, x0, y0, sigma_x, sigma_y)\n",
    "    return (model - data).ravel()\n",
    "\n",
    "# Convert parameters from A, x0, y0, sigma_x, sigma_y --> mean_x, mean_y, cov_x, cov_y, theta\n",
    "# params = A, x0, y0, sigma_x, sigma_y\n",
    "def convert_parameters(parameters):\n",
    "    A, x0, y0, sigma_x, sigma_y = parameters\n",
    "    mean_x = x0\n",
    "    mean_y = y0\n",
    "    cov_x = sigma_x\n",
    "    cov_y = sigma_y\n",
    "\n",
    "    if cov_x != 0 and cov_y != 0:\n",
    "        theta = 0.5 * np.arctan(2 * cov_x * cov_y / (cov_x**2 - cov_y**2)+1e-9)\n",
    "    else:\n",
    "        theta = 0.0\n",
    "\n",
    "    return mean_x, mean_y, cov_x, cov_y, theta\n",
    "\n",
    "@delayed\n",
    "def fit_gaussian_2D_delayed(data, guess):\n",
    "    y, x = np.indices(data.shape)\n",
    "    result = least_squares(residuals, guess, args=(x, y, data))\n",
    "    return result.x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate Labels:\n",
    "load_labels = YES # (Takes <1 min to load, ~40 mins to generate)\n",
    "\n",
    "# Import From File\n",
    "\n",
    "if load_labels:\n",
    "    RHEED_LABEL_FILE = DATA_DIR + 'Labels.npy'\n",
    "    estimated_labels = np.load(RHEED_LABEL_FILE)\n",
    "    # estimated_labels = np.random.rand(normalized_images.shape[0], 5)\n",
    "\n",
    "# Generate\n",
    "else:\n",
    "    estimated_labels = []\n",
    "    with Client() as client:\n",
    "        guesses = [add_guess(image) for image in normalized_images]\n",
    "        fits = [fit_gaussian_2D_delayed(image, guess) for image, guess in zip(normalized_images, guesses)]\n",
    "        estimated_labels = [convert_parameters(params) for params in compute(*fits)]\n",
    "    estimated_labels = np.array(estimated_labels).astype(np.float32)\n",
    "\n",
    "print(f'[Estimated Labels Shape]: {estimated_labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataSet:\n",
    "batch_size = 1000\n",
    "\n",
    "with tf.device('CPU'):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(normalized_images)\n",
    "    dataset = dataset.shuffle(normalized_images.shape[0], reshuffle_each_iteration=True)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "output_scaler.fit(estimated_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Function: (TENSORFLOW)\n",
    "print_example_guassian = NO\n",
    "\n",
    "# mean_x, mean_y, cov_x, cov_y, theta\n",
    "def generate_guassian(batch, image_shape):\n",
    "    batch_size = batch.shape[0]\n",
    "    mean_x, mean_y, cov_x, cov_y, theta = tf.unstack(batch, axis=-1)\n",
    "    x = tf.range(image_shape[1], dtype=tf.float32)[:, tf.newaxis]\n",
    "    x = tf.tile(x, [1, image_shape[0]])\n",
    "\n",
    "    y = tf.range(image_shape[0], dtype=tf.float32)[tf.newaxis, :]\n",
    "    y = tf.tile(y, [image_shape[1], 1])\n",
    "\n",
    "    x = tf.tile(tf.expand_dims(x, 0), [batch_size, 1, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [batch_size, 1, 1])\n",
    "\n",
    "    rota_matrix = tf.stack([tf.cos(theta), -tf.sin(theta), tf.sin(theta), tf.cos(theta)], axis=-1)\n",
    "    rota_matrix = tf.reshape(rota_matrix, (batch_size, 2, 2))\n",
    "\n",
    "    xy = tf.stack([x - tf.reshape(mean_x, (-1, 1, 1)), y - tf.reshape(mean_y, (-1, 1, 1))], axis=-1)\n",
    "    xy = tf.einsum('bijk,bkl->bijl', xy, rota_matrix)\n",
    "\n",
    "    img = tf.exp(-0.5 * (xy[:, :, :, 0]**2 / tf.reshape(cov_x, (-1, 1, 1))**2 + xy[:, :, :, 1]**2 / tf.reshape(cov_y, (-1, 1, 1))**2))\n",
    "\n",
    "    return tf.expand_dims(img, axis=1)\n",
    "\n",
    "if print_example_guassian:\n",
    "    image_shape = (48, 48)\n",
    "    batch = tf.convert_to_tensor([\n",
    "        [21.8558168, 24.50041009, 10.31268177, 9.1700225, 0.72681534]\n",
    "        , [21.76068143, 24.37956637, 10.30043488, 9.15426013, 0.72655111]\n",
    "        , [21.72363929, 24.31050759, 10.33800891, 9.18570812, 0.72644599]\n",
    "        , [21.72777699, 24.29306623, 10.30178808, 9.14728058, 0.72610718]\n",
    "        , [21.79849472, 24.34649405, 10.32683150, 9.16259293, 0.72573213]\n",
    "    ])\n",
    "    generated_imgs = generate_guassian(batch, image_shape)\n",
    "    plt.imshow(tf.squeeze(generated_imgs[0]))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Loss Function (TENSORFLOW):\n",
    "print_example_loss = YES\n",
    "\n",
    "def custom_weighted_mse_loss(I, J, n):\n",
    "  W = tf.pow(I, n)\n",
    "\n",
    "  squared_diffs = tf.pow(I - J, 2)\n",
    "\n",
    "  weighted_squared_diffs = W * squared_diffs\n",
    "\n",
    "  loss = tf.reduce_mean(weighted_squared_diffs)\n",
    "\n",
    "  return loss\n",
    "\n",
    "if print_example_loss:\n",
    "  I = tf.random.normal((5, 1, 48, 48))\n",
    "  J = tf.random.normal((5, 1, 48, 48))\n",
    "  n = 2\n",
    "  loss = custom_weighted_mse_loss(I, J, n)\n",
    "  print(\"[Custom Weighted MSE Loss]:\", loss.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [   \n",
    "        tf.keras.layers.Conv2D(filters=6, kernel_size=5, strides=1, padding='valid', input_shape=(48, 48, 1)) # (batch_size, height, width, channels)\n",
    "        , tf.keras.layers.BatchNormalization(axis=1, momentum=0.1, epsilon=1e-05)\n",
    "        , tf.keras.layers.ReLU()\n",
    "        , tf.keras.layers.MaxPool2D(pool_size=4, strides=4)\n",
    "\n",
    "        , tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='valid')\n",
    "        , tf.keras.layers.BatchNormalization(axis=1, momentum=0.1, epsilon=1e-05)\n",
    "        , tf.keras.layers.ReLU()\n",
    "        , tf.keras.layers.MaxPool2D(pool_size=2, strides=2)\n",
    "\n",
    "        , tf.keras.layers.Flatten()\n",
    "        , tf.keras.layers.Dense(units=98, activation='relu')\n",
    "        , tf.keras.layers.Dense(units=52, activation='relu')\n",
    "        , tf.keras.layers.Dense(units=5)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss=custom_weighted_mse_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "train_model = YES\n",
    "save_model = NO\n",
    "load_model = NO\n",
    "\n",
    "if train_model:\n",
    "    best_loss = float('inf')\n",
    "    num_epochs = 1\n",
    "    lr = 0.0001\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    n = 1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            n += 0.1\n",
    "\n",
    "        for image_batch in tqdm(dataset): \n",
    "            image_batch = tf.expand_dims(image_batch, axis=3) # (batch_size, height, width, channels)\n",
    "            with tf.GradientTape() as tape:\n",
    "                embedding = model(image_batch)\n",
    "                unscaled_param = tf.constant(embedding * output_scaler.var_ ** 0.5 + output_scaler.mean_)\n",
    "                final = generate_guassian(unscaled_param, (48,48))\n",
    "                loss = custom_weighted_mse_loss(image_batch, final, n)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            running_loss += loss.numpy()\n",
    "        average_loss = running_loss / len(dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}\")\n",
    "\n",
    "if save_model:\n",
    "    model.save(\"GaussianModle.keras\")\n",
    "\n",
    "if load_model:\n",
    "    pass\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-Up for Benchmarking:\n",
    "\n",
    "avg_params = np.mean(estimated_labels, axis=0) # output_scaler.mean_ ?\n",
    "std_params = np.std(estimated_labels, axis=0) # output_scaler.var_ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Plausible Gaussians:\n",
    "num_generated_gaussians = 1000\n",
    "\n",
    "generated_gaussians_labels = []\n",
    "for num in range(num_generated_gaussians):\n",
    "    new_gaussian_label = []\n",
    "    for (avg, std) in zip(avg_params, std_params):\n",
    "        new_gaussian_label.append(avg + np.random.normal(loc=0, scale=std))\n",
    "    generated_gaussians_labels.append(new_gaussian_label)\n",
    "generated_gaussians_labels = np.array(generated_gaussians_labels)\n",
    "\n",
    "generated_gaussians_images = generate_guassian(tf.convert_to_tensor(generated_gaussians_labels, dtype=tf.float32), (48,48))\n",
    "\n",
    "print(f'[Generated Gaussian Labels Shape]: {generated_gaussians_labels.shape}')\n",
    "print(f'[Generated Guassian Images Shape]: {generated_gaussians_images.shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hls4ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
