{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Variables: \n",
    "\n",
    "YES = 1\n",
    "NO = 0\n",
    "\n",
    "DATA_DIR = '/home/pujan/Research/RHEED/Data/' # Change to your DATA PATH\n",
    "using_GPU = YES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "%matplotlib inline\n",
    "if(using_GPU):\n",
    "    print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read H5 Data File:\n",
    "RHEED_data_file = DATA_DIR + 'RHEED_4848_test6.h5'\n",
    "spot = 'spot_2'\n",
    "\n",
    "h5 = h5py.File(RHEED_data_file, 'r')\n",
    "\n",
    "raw_data = []\n",
    "for growth in h5.keys():\n",
    "    raw_data.extend(h5[growth][spot])\n",
    "raw_data = np.array(raw_data).astype(np.float32)\n",
    "raw_data = np.expand_dims(raw_data, axis=-1).astype(np.float32) # if (batch_size, height, width, channels)\n",
    "\n",
    "print(f'[Raw Images Shape]: {raw_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize w/ image max\n",
    "\n",
    "normalized_images = []\n",
    "normalized_factor = []\n",
    "for image in tqdm(raw_data):\n",
    "    normalized_images.append(image / (np.max(image) + 1))\n",
    "    normalized_factor.append(np.max(image))\n",
    "normalized_images = np.array(normalized_images).astype(np.float32)\n",
    "normalized_factor = np.array(normalized_factor).astype(np.float32)\n",
    "\n",
    "print(f'[Normalized Images Shape]: {normalized_images.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clamp to 8 bits fractional\n",
    "normalized_images_0I_8F = []\n",
    "for image in tqdm(normalized_images):\n",
    "    normalized_images_0I_8F.append(np.round(image * 256) / 256)\n",
    "normalized_images_0I_8F = np.array(normalized_images_0I_8F).astype(np.float32)\n",
    "\n",
    "print(f'[Normalized Images Shape]: {normalized_images_0I_8F.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataSet:\n",
    "batch_size = 1000\n",
    "dataset_arr = normalized_images_0I_8F\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(dataset_arr)\n",
    "dataset = dataset.shuffle(dataset_arr.shape[0], reshuffle_each_iteration=True)\n",
    "dataset = dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Functions: (TENSORFLOW)\n",
    "print_example_guassian = YES\n",
    "print_example_loss = YES\n",
    "\n",
    "# mean_x, mean_y, cov_x, cov_y, theta\n",
    "def generate_guassian(batch, image_shape):\n",
    "    batch_size = batch.shape[0]\n",
    "    batch = tf.expand_dims(tf.expand_dims(batch, axis=-1), axis=-1)\n",
    "    x0, y0, sigma_x, sigma_y, theta = tf.cast(tf.unstack(batch, axis=-3), tf.float32)\n",
    "    \n",
    "    x_range = tf.range(start=0, limit=image_shape[0], delta=1)\n",
    "    y_range = tf.range(start=0, limit=image_shape[1], delta=1)\n",
    "    X_coord, Y_coord = tf.meshgrid(x_range, y_range, indexing='xy')\n",
    "    X_coord = tf.cast(tf.expand_dims(X_coord, axis=0), tf.float32)\n",
    "    Y_coord = tf.cast(tf.expand_dims(Y_coord, axis=0), tf.float32)\n",
    "    \n",
    "    X_coord = tf.tile(X_coord, [batch_size, 1, 1])\n",
    "    Y_coord = tf.tile(Y_coord, [batch_size, 1, 1])\n",
    "    \n",
    "    a = tf.math.pow(tf.math.cos(theta), 2) / (2 * tf.math.pow(sigma_x, 2)) + tf.math.pow(tf.math.sin(theta), 2) / (2 * tf.math.pow(sigma_y, 2))\n",
    "    b = -1 * tf.math.sin(theta) * tf.math.cos(theta) / (2 * tf.math.pow(sigma_x, 2)) + tf.math.sin(theta) * tf.math.cos(theta) / (2 * tf.math.pow(sigma_y, 2))\n",
    "    c = tf.math.pow(tf.math.sin(theta), 2) / (2 * tf.math.pow(sigma_x, 2)) + tf.math.pow(tf.math.cos(theta), 2) / (2 * tf.math.pow(sigma_y, 2))\n",
    "\n",
    "    img = tf.exp(-1 * (a * (X_coord - x0) ** 2 + 2 * b * (X_coord - x0) * (Y_coord - y0) + c * (Y_coord - y0) ** 2))\n",
    "\n",
    "    return tf.expand_dims(img, axis=-1) # if (batch_size, height, width, channels)\n",
    "    return tf.expand_dims(img, axis=1)  # if (batch_size, channels, height, width)\n",
    "\n",
    "def custom_weighted_mse_loss(I, J, n):\n",
    "  W = tf.pow(I, n)\n",
    "\n",
    "  squared_diffs = tf.pow(I - J, 2)\n",
    "\n",
    "  weighted_squared_diffs = W * squared_diffs\n",
    "\n",
    "  loss = tf.reduce_mean(weighted_squared_diffs)\n",
    "\n",
    "  return loss\n",
    "\n",
    "if print_example_guassian:\n",
    "    image_shape = (48, 48)\n",
    "    batch = tf.convert_to_tensor([\n",
    "        [24, 24, 13, 7, 0],\n",
    "        [24, 20, 13, 7, 0],\n",
    "        [24, 24, 13, 7, np.pi/2],\n",
    "    ])\n",
    "\n",
    "    generated_imgs = generate_guassian(batch, image_shape)\n",
    "    for i in range(len(batch)):\n",
    "        plt.subplot(1, len(batch), i + 1)\n",
    "        plt.imshow(tf.squeeze(generated_imgs[i]))\n",
    "    plt.show()\n",
    "\n",
    "if print_example_loss:\n",
    "    I = tf.random.normal((5, 1, 48, 48))\n",
    "    J = tf.random.normal((5, 1, 48, 48))\n",
    "    n = 2\n",
    "    loss = custom_weighted_mse_loss(I, J, n)\n",
    "    print(\"[Custom Weighted MSE Loss]:\", loss.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Conv2D(filters=6, kernel_size=5, strides=1, padding='valid'),\n",
    "        tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-05),  # if (batch_size, height, width, channels)\n",
    "        # tf.keras.layers.BatchNormalization(axis=1, momentum=0.1, epsilon=1e-05), # if (batch_size, channels, height, width)\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=4, strides=4),\n",
    "\n",
    "        tf.keras.layers.Conv2D(filters=16, kernel_size=5, strides=1, padding='valid'),\n",
    "        tf.keras.layers.BatchNormalization(axis=-1, momentum=0.1, epsilon=1e-05), # if (batch_size, height, width, channels)\n",
    "        # tf.keras.layers.BatchNormalization(axis=1, momentum=0.1, epsilon=1e-05), # if (batch_size, channels, height, width)\n",
    "        tf.keras.layers.ReLU(),\n",
    "        tf.keras.layers.MaxPool2D(pool_size=2, strides=2),\n",
    "\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(units=98, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=52, activation='relu'),\n",
    "        tf.keras.layers.Dense(units=5)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "train_model = YES\n",
    "save_model = YES\n",
    "load_model = NO\n",
    "\n",
    "if train_model:\n",
    "    num_epochs = 200\n",
    "    n = 1\n",
    "\n",
    "    # lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    #     initial_learning_rate=0.01,\n",
    "    #     decay_steps=num_epochs,\n",
    "    #     decay_rate=0.9)\n",
    "    \n",
    "    lr_schedule = 0.0001\n",
    "    adam_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "    \n",
    "    scaling_arr = tf.constant([48.0, 48.0, 24.0, 24.0, np.pi/2])\n",
    "\n",
    "    loss_arr = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            n += 0.1\n",
    "        \n",
    "        for image_batch in tqdm(dataset): \n",
    "            with tf.GradientTape() as tape:\n",
    "                embedding = model(image_batch)\n",
    "                unscaled_param = tf.constant(embedding * scaling_arr)\n",
    "                final = generate_guassian(unscaled_param, (48, 48))\n",
    "                loss = custom_weighted_mse_loss(image_batch, final, n)\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            adam_optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            running_loss += loss.numpy()\n",
    "        average_loss = running_loss / len(dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {average_loss}\")\n",
    "        loss_arr.append(average_loss)\n",
    "    loss_arr = np.array(loss_arr)\n",
    "\n",
    "if save_model:\n",
    "    model.save(f'{DATA_DIR}/Models/[32]Gaussian.keras')\n",
    "\n",
    "if load_model:\n",
    "    model = tf.keras.models.load_model(\"Gaussian_Model.keras\")\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RHEED",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
